{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQGLt1cLsLJ_"
      },
      "outputs": [],
      "source": [
        "#GROUP 10:\n",
        "#Shaun Jacob Varghese: 20BAC10022\n",
        "#Varun Ram S: 20BAC10038\n",
        "#Manoshi Raha: 20BAC10020\n",
        "#Jenish Murdia: 20BAC10004\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2 as cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import MultiHeadAttention"
      ],
      "metadata": {
        "id": "WrehFisZfgXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers.merge import concatenate"
      ],
      "metadata": {
        "id": "pnqodc7agWP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPool2D\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Dense\n",
        "#from tensorflow.keras.layers.Attention (use_scale=False,score_mode=\"dot\",**kwargs)\n"
      ],
      "metadata": {
        "id": "ZP1-1VJ61GsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import cifar10\n",
        "(trainx,trainy),(testx,testy) = cifar10.load_data()\n",
        "\n",
        "print(\"Train of X and Y is:\",(trainx.shape),(trainy.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3DegUz86-DG",
        "outputId": "e8999f0d-479e-457a-ad48-0ff108e27784"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 11s 0us/step\n",
            "170508288/170498071 [==============================] - 11s 0us/step\n",
            "Train of X and Y is: (50000, 32, 32, 3) (50000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Test of X and Y is:\",(testx.shape),(testy.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zM1gZEw97Y6y",
        "outputId": "0d7c19ca-7dc3-4c78-a988-acd4704786cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test of X and Y is: (10000, 32, 32, 3) (10000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainx = trainx.astype('float32') \n",
        "testx = testx.astype('float32') \n",
        "trainx = trainx / 255.0 \n",
        "testx = testx / 255.0\n",
        "\n",
        "from keras.utils import np_utils \n",
        "trainy = np_utils.to_categorical(trainy) \n",
        "testy = np_utils.to_categorical(testy) \n",
        "num_classes = testy.shape[1]\n",
        "\n"
      ],
      "metadata": {
        "id": "ueqzAc8r7iZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input"
      ],
      "metadata": {
        "id": "s49W7AF0gowJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp=Input(shape=(64,64,1))\n",
        "\n",
        "c1=keras.layers.Conv2D(32, (7, 7), input_shape=(32,32,7), activation='relu', padding='same')(temp)\n",
        "p1=keras.layers.MaxPool2D(pool_size=(2, 2))(c1)\n",
        "\n",
        "\n",
        "x1=keras.layers.Conv2D(64,(1,1),input_shape=(64,64,1), activation='relu', padding='same')(p1)\n",
        "x2=keras.layers.Conv2D(64,(3,3),input_shape=(64,64,3), activation='relu', padding='same')(p1)\n",
        "x3=keras.layers.Conv2D(64,(5,5),input_shape=(64,64,5), activation='relu', padding='same')(p1)\n",
        "x4=keras.layers.MaxPool2D(pool_size=(2,1))(p1)\n",
        "x5=keras.layers.Conv2D(64,(1,1))(x4)\n",
        "\"\"\"\"\n",
        "y1=keras.layers.MultiHeadAttention(num_heads=1,key_dim=4)(x1)\n",
        "y2=keras.layers.MultiHeadAttention(num_heads=1,key_dim=4)(x2)\n",
        "y3=keras.layers.MultiHeadAttention(num_heads=1,key_dim=4)(x3)\n",
        "y4=keras.layers.MultiHeadAttention(num_heads=4,key_dim=4)(x5)\n",
        "\n",
        "merge1=concatenate([y1,y2,y3,y4])\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "rUAHZYvv4h9W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "12bc02ee-ae3a-42f8-b867-b424618b657c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\"\\ny1=keras.layers.MultiHeadAttention(num_heads=1,key_dim=4)(x1)\\ny2=keras.layers.MultiHeadAttention(num_heads=1,key_dim=4)(x2)\\ny3=keras.layers.MultiHeadAttention(num_heads=1,key_dim=4)(x3)\\ny4=keras.layers.MultiHeadAttention(num_heads=4,key_dim=4)(x5)\\n\\nmerge1=concatenate([y1,y2,y3,y4])\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from scipy.special import softmax"
      ],
      "metadata": {
        "id": "hOfG_BIyYi7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "def att(y1,y2,y3,y5):\n",
        "  p_layers=[y1,y2,y3,y5]\n",
        "  p_layers.sort()\n",
        "  result=[]\n",
        "  for i in range(len(p_layers)):\n",
        "    result.append(p_layers[i]+softmax(p_layers[i+1]*p_layers[i-1]) + p_layers[i])\n",
        "  \n",
        "  layer=Sequential()\n",
        "  for i in range(1,len(result)):\n",
        "    result[0].concat(result[i])\n",
        "  final=result[0]\n",
        "  return final\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "WBuTeoEeWSpq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "4ca97d61-5860-44ad-abc1-51b7173c9430"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef att(y1,y2,y3,y5):\\n  p_layers=[y1,y2,y3,y5]\\n  p_layers.sort()\\n  result=[]\\n  for i in range(len(p_layers)):\\n    result.append(p_layers[i]+softmax(p_layers[i+1]*p_layers[i-1]) + p_layers[i])\\n  \\n  layer=Sequential()\\n  for i in range(1,len(result)):\\n    result[0].concat(result[i])\\n  final=result[0]\\n  return final\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We will flatten after getting I1 to I4\n",
        "y1=keras.layers.Flatten()(x1)\n",
        "y2=keras.layers.Flatten()(x2)\n",
        "y3=keras.layers.Flatten()(x3)\n",
        "y5=keras.layers.Flatten()(x5)\n",
        "#P(H,W,C) ppart of the research paper\n",
        "par1_1=keras.layers.Dense(64,activation='relu')(y1)\n",
        "par1_2=keras.layers.Dense(64,activation='relu')(y1)\n",
        "par1_3=keras.layers.Dense(64,activation='relu')(y1)\n",
        "\n",
        "par2_1=keras.layers.Dense(64,activation='relu')(y2)\n",
        "par2_2=keras.layers.Dense(64,activation='relu')(y2)\n",
        "par2_3=keras.layers.Dense(64,activation='relu')(y2)\n",
        "\n",
        "par3_1=keras.layers.Dense(64,activation='relu')(y3)\n",
        "par3_2=keras.layers.Dense(64,activation='relu')(y3)\n",
        "par3_3=keras.layers.Dense(64,activation='relu')(y3)\n",
        "\n",
        "par4_1=keras.layers.Dense(64,activation='relu')(y5)\n",
        "par4_2=keras.layers.Dense(64,activation='relu')(y5)\n",
        "par4_3=keras.layers.Dense(64,activation='relu')(y5)\n"
      ],
      "metadata": {
        "id": "fp1R68ygdKPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "P1at = tf.matmul(par1_2,par1_3)\n",
        "sm1_1 = tf.keras.activations.softmax(P1at, axis=-1)\n",
        "sm1_2 = tf.matmul(par1_1,sm1_1)\n",
        "#tf.keras.layers.Add(sm1_2,x1)\n",
        "\n",
        "P2at = tf.matmul(par2_2,par2_3)\n",
        "sm2_1 = tf.keras.activations.softmax(P2at, axis=-1)\n",
        "sm2_2 = tf.matmul(par2_1,sm2_1)\n",
        "#tf.keras.layers.Add(sm2_2,x2)\n",
        "\n",
        "P3at = tf.matmul(par3_2,par3_3)\n",
        "sm3_1 = tf.keras.activations.softmax(P3at, axis=-1)\n",
        "sm3_2 = tf.matmul(par3_1,sm3_1)\n",
        "#tf.keras.layers.Add(sm3_2,x3)\n",
        "\n",
        "P4at =tf.matmul(par4_2,par4_3)\n",
        "sm4_1 = tf.keras.activations.softmax(P4at, axis=-1)\n",
        "sm4_2 = tf.matmul(par4_1,sm4_1)\n",
        "#tf.keras.layers.Add(sm4_2,x4)\n",
        "\n",
        "fin1=keras.layers.Flatten()(sm1_2)\n",
        "#fin_1=tf.keras.layers.Add()([fin1,y1])\n",
        "fin2=keras.layers.Flatten()(sm2_2)\n",
        "fin3=keras.layers.Flatten()(sm3_2)\n",
        "fin4=keras.layers.Flatten()(sm4_2)\n"
      ],
      "metadata": {
        "id": "ieKytuOrMJMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = concatenate([fin1,fin2,fin3,fin4])\n",
        "model = Model(inputs=temp, outputs=output)"
      ],
      "metadata": {
        "id": "3fqyJI0knuvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZhqeG49OZ7k",
        "outputId": "bbe43b0c-fa20-4f20-b588-0173e7acbd32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 64, 64, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 64, 64, 32)   1600        ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 32)  0           ['conv2d_5[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling2d_3 (MaxPooling2D)  (None, 16, 32, 32)  0           ['max_pooling2d_2[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 32, 32, 64)   2112        ['max_pooling2d_2[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 32, 32, 64)   18496       ['max_pooling2d_2[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 32, 32, 64)   51264       ['max_pooling2d_2[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 16, 32, 64)   2112        ['max_pooling2d_3[0][0]']        \n",
            "                                                                                                  \n",
            " flatten_4 (Flatten)            (None, 65536)        0           ['conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            " flatten_5 (Flatten)            (None, 65536)        0           ['conv2d_7[0][0]']               \n",
            "                                                                                                  \n",
            " flatten_6 (Flatten)            (None, 65536)        0           ['conv2d_8[0][0]']               \n",
            "                                                                                                  \n",
            " flatten_7 (Flatten)            (None, 32768)        0           ['conv2d_9[0][0]']               \n",
            "                                                                                                  \n",
            " dense_13 (Dense)               (None, 64)           4194368     ['flatten_4[0][0]']              \n",
            "                                                                                                  \n",
            " dense_14 (Dense)               (None, 64)           4194368     ['flatten_4[0][0]']              \n",
            "                                                                                                  \n",
            " dense_16 (Dense)               (None, 64)           4194368     ['flatten_5[0][0]']              \n",
            "                                                                                                  \n",
            " dense_17 (Dense)               (None, 64)           4194368     ['flatten_5[0][0]']              \n",
            "                                                                                                  \n",
            " dense_19 (Dense)               (None, 64)           4194368     ['flatten_6[0][0]']              \n",
            "                                                                                                  \n",
            " dense_20 (Dense)               (None, 64)           4194368     ['flatten_6[0][0]']              \n",
            "                                                                                                  \n",
            " dense_22 (Dense)               (None, 64)           2097216     ['flatten_7[0][0]']              \n",
            "                                                                                                  \n",
            " dense_23 (Dense)               (None, 64)           2097216     ['flatten_7[0][0]']              \n",
            "                                                                                                  \n",
            " tf.linalg.matmul_4 (TFOpLambda  (None, 64)          0           ['dense_13[0][0]',               \n",
            " )                                                                'dense_14[0][0]']               \n",
            "                                                                                                  \n",
            " tf.linalg.matmul_6 (TFOpLambda  (None, 64)          0           ['dense_16[0][0]',               \n",
            " )                                                                'dense_17[0][0]']               \n",
            "                                                                                                  \n",
            " tf.linalg.matmul_8 (TFOpLambda  (None, 64)          0           ['dense_19[0][0]',               \n",
            " )                                                                'dense_20[0][0]']               \n",
            "                                                                                                  \n",
            " tf.linalg.matmul_10 (TFOpLambd  (None, 64)          0           ['dense_22[0][0]',               \n",
            " a)                                                               'dense_23[0][0]']               \n",
            "                                                                                                  \n",
            " dense_12 (Dense)               (None, 64)           4194368     ['flatten_4[0][0]']              \n",
            "                                                                                                  \n",
            " tf.nn.softmax (TFOpLambda)     (None, 64)           0           ['tf.linalg.matmul_4[0][0]']     \n",
            "                                                                                                  \n",
            " dense_15 (Dense)               (None, 64)           4194368     ['flatten_5[0][0]']              \n",
            "                                                                                                  \n",
            " tf.nn.softmax_1 (TFOpLambda)   (None, 64)           0           ['tf.linalg.matmul_6[0][0]']     \n",
            "                                                                                                  \n",
            " dense_18 (Dense)               (None, 64)           4194368     ['flatten_6[0][0]']              \n",
            "                                                                                                  \n",
            " tf.nn.softmax_2 (TFOpLambda)   (None, 64)           0           ['tf.linalg.matmul_8[0][0]']     \n",
            "                                                                                                  \n",
            " dense_21 (Dense)               (None, 64)           2097216     ['flatten_7[0][0]']              \n",
            "                                                                                                  \n",
            " tf.nn.softmax_3 (TFOpLambda)   (None, 64)           0           ['tf.linalg.matmul_10[0][0]']    \n",
            "                                                                                                  \n",
            " tf.linalg.matmul_5 (TFOpLambda  (None, 64)          0           ['dense_12[0][0]',               \n",
            " )                                                                'tf.nn.softmax[0][0]']          \n",
            "                                                                                                  \n",
            " tf.linalg.matmul_7 (TFOpLambda  (None, 64)          0           ['dense_15[0][0]',               \n",
            " )                                                                'tf.nn.softmax_1[0][0]']        \n",
            "                                                                                                  \n",
            " tf.linalg.matmul_9 (TFOpLambda  (None, 64)          0           ['dense_18[0][0]',               \n",
            " )                                                                'tf.nn.softmax_2[0][0]']        \n",
            "                                                                                                  \n",
            " tf.linalg.matmul_11 (TFOpLambd  (None, 64)          0           ['dense_21[0][0]',               \n",
            " a)                                                               'tf.nn.softmax_3[0][0]']        \n",
            "                                                                                                  \n",
            " flatten_8 (Flatten)            (None, 64)           0           ['tf.linalg.matmul_5[0][0]']     \n",
            "                                                                                                  \n",
            " flatten_9 (Flatten)            (None, 64)           0           ['tf.linalg.matmul_7[0][0]']     \n",
            "                                                                                                  \n",
            " flatten_10 (Flatten)           (None, 64)           0           ['tf.linalg.matmul_9[0][0]']     \n",
            "                                                                                                  \n",
            " flatten_11 (Flatten)           (None, 64)           0           ['tf.linalg.matmul_11[0][0]']    \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 256)          0           ['flatten_8[0][0]',              \n",
            "                                                                  'flatten_9[0][0]',              \n",
            "                                                                  'flatten_10[0][0]',             \n",
            "                                                                  'flatten_11[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 44,116,544\n",
            "Trainable params: 44,116,544\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "#Defining attention mechanism\n",
        "class attention(layer):\n",
        "  def __init__(self, return_sequences = True):\n",
        "    self.retun_sequences = return_sequences\n",
        "\n",
        "    super(attention,self).__init__()\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.W= self.add_weight(name=\"att_weight\", shape=(input_shape[-1],1), initializer=\"normal\")\n",
        "    self.b= self.add_weight(name=\"att_bias\", shape=(input_shape[1],1), initializer=\"normal\")\n",
        "    self.b= self.add_weight(name=\"att_bias\", shape=(input_shape[1],1))\n",
        "    self.b= self.add_weight(name=\"att_bias\", shape=(input_shape[1],1))\n",
        "    super(attention,self).build(input_shape)\n",
        "\n",
        "  def call(self,x):\n",
        "    e = K.tanh(K.dot(x,self.W)+self.b)\n",
        "    a = K.softmax(e, axis=1)\n",
        "    output = x*a\n",
        "    if self.return_sequences:\n",
        "      return output\n",
        "    return K.sum(output, axis=1)\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.W= self.add_weight(name=\"att_weight\", shape=(input_shape[-1],1), initializer=\"normal\")\n",
        "    self.b= self.add_weight(name=\"att_bias\", shape=(input_shape[1],1), initializer=\"zeros\")\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "TnmpgpGFayQ1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "0b9d7099-861d-4142-eb9c-87c757d463e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n#Defining attention mechanism\\nclass attention(layer):\\n  def __init__(self, return_sequences = True):\\n    self.retun_sequences = return_sequences\\n\\n    super(attention,self).__init__()\\n\\n  def build(self, input_shape):\\n    self.W= self.add_weight(name=\"att_weight\", shape=(input_shape[-1],1), initializer=\"normal\")\\n    self.b= self.add_weight(name=\"att_bias\", shape=(input_shape[1],1), initializer=\"normal\")\\n    self.b= self.add_weight(name=\"att_bias\", shape=(input_shape[1],1))\\n    self.b= self.add_weight(name=\"att_bias\", shape=(input_shape[1],1))\\n    super(attention,self).build(input_shape)\\n\\n  def call(self,x):\\n    e = K.tanh(K.dot(x,self.W)+self.b)\\n    a = K.softmax(e, axis=1)\\n    output = x*a\\n    if self.return_sequences:\\n      return output\\n    return K.sum(output, axis=1)\\n\\n  def build(self, input_shape):\\n    self.W= self.add_weight(name=\"att_weight\", shape=(input_shape[-1],1), initializer=\"normal\")\\n    self.b= self.add_weight(name=\"att_bias\", shape=(input_shape[1],1), initializer=\"zeros\")\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    }
  ]
}